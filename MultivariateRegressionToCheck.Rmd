```markdown
---
title: "Regression Analysis"
author: "Your Name"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#analisi di regressione
# descrizione di questa sezione
Questa parte si occupa dell'analisi di regressione multivariata effettuata sul dataset House Prices (train.csv).

Gli step effettuati sono il preprocessing, nel quale vengono puliti i dati mancanti, e rimosse le variabili categoriche con un solo livello (in quanto causavano problemi nella regressione).

Viene utilizziato un approccio STEPWISE con AIC come criterio di informazione per la decisione del modello migliore, poi vengono anaizzati residui con qqplot, distanza di crook e leva, in modo da comprendere se c'è margine di miglioramento del modello, e se qualche outlier o dato improprio andrebbe rimosso per migliorarne le prestazioni e la generalizzazione.


# caricamento dei pacchetti

```{r load-libraries}
library(MASS)
library(car)
library(ggplot2)
library(ggfortify)
library(dplyr)
library(lmtest)
```

# analisi di regressione del dataset completo

## caricamento del dataset

```{r load-dataset}
data <- read.csv('train.csv')
data$Id <- NULL
```

## Data Preprocessing

### gestione dei valori mancanti

```{r handle-missing-values}
for(col in names(data)) {
  if (is.numeric(data[[col]])) {
    data[[col]][is.na(data[[col]])] <- mean(data[[col]], na.rm = TRUE)
  } else {
    data[[col]][is.na(data[[col]])] <- as.character(sort(table(data[[col]])[1]))
  }
}
```

### identificazione e rimozione dei valori categorici singoli

```{r remove-single-level-categorical}
categorical_vars <- names(data)[sapply(data, function(x) is.factor(x) | is.character(x))]
single_level_vars <- categorical_vars[sapply(data[categorical_vars], function(x) length(unique(x)) == 1)]
data <- data[, !names(data) %in% single_level_vars]
categorical_vars <- names(data)[sapply(data, function(x) is.factor(x) | is.character(x))]
data[categorical_vars] <- lapply(data[categorical_vars], as.factor)
```

## analisi di correlazione

```{r correlation-analysis}
numericVars <- which(sapply(data, is.numeric))
numericData <- data[, numericVars]
correlations <- cor(numericData, use = "pairwise.complete.obs")
correlationWithSalePrice <- sort(correlations[, 'SalePrice'], decreasing = TRUE)
topNumericPredictors <- names(correlationWithSalePrice)[2:11]
```

## selezione delle feature

```{r feature-selection}
keyCategoricalVars <- c("Neighborhood", "BldgType", "HouseStyle", "OverallQual")
keyCategoricalVars <- keyCategoricalVars[keyCategoricalVars %in% names(data)]
encoded_data <- data.frame(model.matrix(~.-1, data))
encoded_categoricalVars <- grep(paste(keyCategoricalVars, collapse="|"), names(encoded_data), value=TRUE)
selected_predictors <- c(topNumericPredictors, encoded_categoricalVars)
X_selected <- encoded_data[, selected_predictors]
y <- encoded_data$SalePrice
```

## regressione con STEPWISE e AIC

```{r stepwise-regression}
simple.model <- lm(y ~ ., data = X_selected)
stepwise.model <- stepAIC(simple.model, direction = "both", trace = TRUE)
summary(stepwise.model)
```

## Diagnostiche

### valori dei residui vs fitted

```{r residuals-vs-fitted}
residuals <- resid(stepwise.model)
fitted_values <- fitted(stepwise.model)

ggplot() + 
  geom_point(aes(x = fitted_values, y = residuals), color = "blue") + 
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") + 
  ggtitle("Residuals vs Fitted Values") + 
  xlab("Fitted Values") + 
  ylab("Residuals")
```

### Q-Q Plot dei residui

```{r qq-plot}
ggplot() + 
  geom_qq(aes(sample = residuals)) + 
  geom_qq_line(aes(sample = residuals), color = "red") + 
  ggtitle("Q-Q Plot dei residui")
```

### Resudui standard, leva e distanza di Cook

```{r model-diagnostics}
autoplot(stepwise.model, which = 1:6)
```

#descrizione di quanto fatto
in questo primo tentativo di regressione abbiamo ottenuto un R^2 = 0.82, quindi siamo riusciti a spiegare l'82% della variabilità di y (salesprice), il che è un ottimo risultato di partenza, in tutte le regressioni sono state prese le prime 10 variabili piu' correlate a salesprice, unite alle variabili "Neighborhood", "BldgType", "HouseStyle", "OverallQual", inquanto mi sembravano molto importanti, i motivi della scelta ridotta di variabili sono anche di natura computazionale.
dal grafico scatter dei residui si nota che nella parte destra si forma un pattern a "funnel" , che si potrebbe spigare col fatto che c'è della non linearità, o degli outlier che influenzano i risultati del modello.
Si nota subito che il valore 1299 del dataset ha un alto valore di Crook, e un alta leva, quindi è opportuno investigarlo.
Da investigazioni sul dato emerge che il fatto che il SaleCondition di quell ossevazione è  "Partial", il che s'ignifica che le caratteristiche della casa erano state rilevate quando essa era in fase di costruzione, percio' ho rimosso le oservazioni col valore di SaleCondition "Partial", in quanto disturbavano i parametri del modello, e non possiamo sapere con certezza se i parametri di quella casa sono affidabili per spiegare il saleprice che cerchiamo di predire.

# Analysis del dataset senza i SaleCondition "Partial"

```{r filter-partial-sales}
data <- read.csv('train.csv')
data <- data[data$SaleCondition != "Partial", ]
data$Id <- NULL

for(col in names(data)) {
  if (is.numeric(data[[col]])) {
    data[[col]][is.na(data[[col]])] <- mean(data[[col]], na.rm = TRUE)
  } else {
    data[[col]][is.na(data[[col]])] <- as.character(sort(table(data[[col]])[1]))
  }
}

categorical_vars <- names(data)[sapply(data, function(x) is.factor(x) | is.character(x))]
single_level_vars <- categorical_vars[sapply(data[categorical_vars], function(x) length(unique(x)) == 1)]
data <- data[, !names(data) %in% single_level_vars]
categorical_vars <- names(data)[sapply(data, function(x) is.factor(x) | is.character(x))]
data[categorical_vars] <- lapply(data[categorical_vars], as.factor)
numericVars <- which(sapply(data, is.numeric))
numericData <- data[, numericVars]
correlations <- cor(numericData, use = "pairwise.complete.obs")
correlationWithSalePrice <- sort(correlations[, 'SalePrice'], decreasing = TRUE)
topNumericPredictors <- names(correlationWithSalePrice)[2:11]
keyCategoricalVars <- c("Neighborhood", "BldgType", "HouseStyle", "OverallQual")
keyCategoricalVars <- keyCategoricalVars[keyCategoricalVars %in% names(data)]
encoded_data <- data.frame(model.matrix(~.-1, data))
encoded_categoricalVars <- grep(paste(keyCategoricalVars, collapse="|"), names(encoded_data), value=TRUE)
selected_predictors <- c(topNumericPredictors, encoded_categoricalVars)
X_selected <- encoded_data[, selected_predictors]
y <- encoded_data$SalePrice
simple.model <- lm(y ~ ., data = X_selected)
stepwise.model <- stepAIC(simple.model, direction = "both", trace = TRUE)
summary(stepwise.model)
```

## valori dei residui vs fitted (senza Partial Sales)

```{r residuals-vs-fitted-partial}
residuals <- resid(stepwise.model)
fitted_values <- fitted(stepwise.model)

ggplot() + 
  geom_point(aes(x = fitted_values, y = residuals), color = "blue") + 
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") + 
  ggtitle("Residuals vs Fitted Values (Without Partial Sales)") + 
  xlab("Fitted Values") + 
  ylab("Residuals")
```

## Q-Q Plot dei residui (senza Partial Sales)

```{r qq-plot-partial}
ggplot() + 
  geom_qq(aes(sample = residuals)) + 
  geom_qq_line(aes(sample = residuals), color = "red") + 
  ggtitle("Q-Q Plot dei residui (Without Partial Sales)")
```

## Resudui standard, leva e distanza di Cook (senza Partial Sales)

```{r model-diagnostics-partial}
autoplot(stepwise.model, which = 1:6)
```
#descrizione di quanto fatto
in questo secondo tentativo di regressione abbiamo ottenuto un R^2 = 0.85, quindi siamo riusciti a spiegare l'85% della variabilità di y (salesprice), il che è un ottimo incremento, che conferma che il nostro modello è adatto per spiegare le case con condizioni di vendita non "Partial", 
dal grafico scatter dei residui si nota che nella parte destra il pattern precedente si è affievolito, pero' viene mostrata un esplosione dei residui con l'aumentare di y, il che implica che il modello non riesce a generalizzare bene per alti prezzi di saleprice , per tale motivo ho deciso di tentare successivamente una regressione con log(y) come variabile target, per cercare di catturare anche questo tipo di curva dei residui e mitigarla. Il QQ plot dei residui mostra che sulle code si hanno delle deviazioni dalla normalità, presupponendo presenza di outlier o non-linearita.


# Analisi con Log(y) of del Dataset senza Partial Sales

```{r log-transformation-partial}
y_transformed <- log(y)
simple.model <- lm(y_transformed ~ ., data = X_selected)
stepwise.model <- stepAIC(simple.model, direction = "both", trace = TRUE)
summary(stepwise.model)
residuals <- resid(stepwise.model)
fitted_values <- fitted(stepwise.model)

ggplot() + 
  geom_point(aes(x = fitted_values, y = residuals), color = "blue") + 
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") + 
  ggtitle("Residuals vs Fitted Values") + 
  xlab("Fitted Values") + 
  ylab("Residuals")

ggplot() + 
  geom_qq(aes(sample = residuals)) + 
  geom_qq_line(aes(sample = residuals), color = "red") + 
  ggtitle("Q-Q Plot dei residui")

autoplot(stepwise.model, which = 1:6)
dwtest(stepwise.model)
```
#descrizione di quanto fatto
in questo terzo tentativo di regressione abbiamo ottenuto un R^2 = 0.87, quindi siamo riusciti a spiegare l'87% della variabilità di y (salesprice), il che è un buon incremento rispetto a prima, infatti quando si controlla lo scatterplot dei residui, si nota che il pattern di prima è sparito, ci sono ancora alcune non linearita o outlier sulla coda sinistra, che si notano anche nel qq plot. La distanza di Crook e la leva mostrano che non ci sono parametri con un alta distanza di Crook con anche varia Leva sui paraemtri del modello, il che è un buon risoltato per la quantità di parametri limitati.

Ho voluto cercare di ri-computare il modello usando solo i dati di vendite dove la SaleCondition era "Normal", quindi non c'erano vendite tra familiari, o vendite di appartamenti/case multiple, o assestamenti parziali delle caratteristiche delle case.


# Analisi del Dataset con solo Normal Sales

```{r filter-normal-sales}
data <- read.csv('train.csv')
data <- data[data$SaleCondition == "Normal", ]
data$Id <- NULL

for(col in names(data)) {
  if (is.numeric(data[[col]])) {
    data[[col]][is.na(data[[col]])] <- mean(data[[col]], na.rm = TRUE)
  } else {
    data[[col]][is.na(data[[col]])] <- as.character(sort(table(data[[col]])[1]))
  }
}

categorical_vars <- names(data)[sapply(data, function(x) is.factor(x) | is.character(x))]
single_level_vars <- categorical_vars[sapply(data[categorical_vars], function(x) length(unique(x)) == 1)]
data <- data[, !names(data) %in% single_level_vars]
categorical_vars <- names(data)[sapply(data, function(x) is.factor(x) | is.character(x))]
data[categorical_vars] <- lapply(data[categorical_vars], as.factor)
numericVars <- which(sapply(data, is.numeric))
numericData <- data[, numericVars]
correlations <- cor(numericData, use = "pairwise.complete.obs")
correlationWithSalePrice <- sort(correlations[, 'SalePrice'], decreasing = TRUE)
topNumericPredictors <- names(correlationWithSalePrice)[2:11]
keyCategoricalVars <- c("Neighborhood", "BldgType", "HouseStyle", "OverallQual")
keyCategoricalVars <- keyCategoricalVars[keyCategoricalVars %in% names(data)]
encoded_data <- data.frame(model.matrix(~.-1, data))
encoded_categoricalVars <- grep(paste(keyCategoricalVars, collapse="|"), names(encoded_data), value=TRUE)
selected_predictors <- c(topNumericPredictors, encoded_categoricalVars)
X_selected <- encoded_data[, selected_predictors]
y <- encoded_data$SalePrice
simple.model <- lm(y ~ ., data = X_selected)
stepwise.model <- stepAIC(simple.model, direction = "both", trace = TRUE)
summary(stepwise.model)
```

## valori dei residui vs fitted (solo Normal Sales )

```{r residuals-vs-fitted-normal}
residuals <- resid(stepwise.model)
fitted_values <- fitted(stepwise.model)

ggplot() + 
  geom_point(aes(x = fitted_values, y = residuals), color = "blue") + 
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") + 
  ggtitle("Residuals vs Fitted Values (Normal Sales Only)") + 
  xlab("Fitted Values") + 
  ylab("Residuals")
```

## Q-Q Plot dei residui (solo Normal Sales)

```{r qq-plot-normal}
ggplot() + 
  geom_qq(aes(sample = residuals)) + 
  geom_qq_line(aes(sample = residuals), color = "red") + 
  ggtitle("Q-Q Plot of Residuals (Normal Sales Only)")
```

## Resudui standard, leva e distanza di Cook (solo Normal Sales )

```{r model-diagnostics-normal}
autoplot(stepwise.model, which = 1:6)
```


# Analili con Log(y) del Dataset con solo Normal Sales

```{r log-transformation-normal}
y_transformed <- log(y)
simple.model <- lm(y_transformed ~ ., data = X_selected)
stepwise.model <- stepAIC(simple.model, direction = "both", trace = TRUE)
summary(stepwise.model)
residuals <- resid(stepwise.model)
fitted_values <- fitted(stepwise.model)

ggplot() + 
  geom_point(aes(x = fitted_values, y = residuals), color = "blue") + 
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") + 
  ggtitle("Residuals vs Fitted Values") + 
  xlab("Fitted Values") + 
  ylab("Residuals")

ggplot() + 
  geom_qq(aes(sample = residuals)) + 
  geom_qq_line(aes(sample = residuals), color = "red") + 
  ggtitle("Q-Q Plot of Residuals")

autoplot(stepwise.model, which = 1:6)
dwtest(stepwise.model)
```
#conclusione tentativo
Il modello performa ancora meglio, con un R^2 di 0.89, quindi un ottima accuratezza sui dati di training con salescondition "Normal".



```
